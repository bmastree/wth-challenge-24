{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 04-B - Generación Aumentada con Recuperación (RAG) para Datos No Estructurados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introducción\n",
    "\n",
    "Las empresas tienen mucha información propietaria que debe tenerse en cuenta al responder las preguntas de los usuarios; estas no siempre pueden ser respondidas a través de los datos con los que se han entrenado los modelos GPT.\n",
    "\n",
    "En el último notebook, trabajamos principalmente con datos estructurados. Muchas veces, los datos de tu empresa no se limitan solo a formatos estructurados como archivos CSV o tablas SQL. También pueden incluir datos no estructurados como documentos PDF o imágenes. De hecho, tus documentos individuales podrían tener tanto datos no estructurados como estructurados integrados. Extraer información de estos formatos diversos de una manera comprensible presenta un desafío. Herramientas como Azure Document Intelligence permiten la extracción de datos de fuentes no estructuradas como formularios o documentos. Una vez que los datos se extraen en un formato JSON estructurado, se puede utilizar AI Search para consolidar toda la información de diferentes tipos de datos en índices, facilitando la recuperación de documentos relevantes.\n",
    "\n",
    "En este notebook, te guiaremos a través de un caso de uso de Generación Aumentada con Recuperación (RAG) que implica trabajar con datos no estructurados. El enfoque RAG combina varias tecnologías para mejorar la calidad y la relevancia de las salidas generadas. Aprovecharemos Azure Document Intelligence para procesar documentos complejos, utilizando la API de layout para extraer texto y tablas de manera efectiva. Utilizaremos Azure AI Search para crear un índice configurando capacidades de búsqueda semántica, lo que permite la recuperación de páginas de documentos relevantes. Además, se incorporarán embeddings para recuperar contenido que esté lo más alineado posible con la pregunta del usuario. Finalmente, el modelo ChatGPT de Azure OpenAI utilizará el contenido extraído para generar una respuesta más significativa. Es importante enfatizar que este proceso de grounding (fundamentación) sigue el patrón RAG mencionado en el cuaderno anterior y ayuda a eliminar inexactitudes en las respuestas generadas.\n",
    "\n",
    "Tus objetivos para este desafío son leer este notebook, ejecutar cada bloque de código, observar los resultados y luego poder responder las preguntas planteadas en la guía del estudiante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tiktoken==0.5.2) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from tiktoken==0.5.2) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install \"tiktoken==0.5.2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Azure Forms Recognizer, Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** La ruta en la celda de código a continuación se refiere a la carpeta `/data/unstructured/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- raw data\n",
    "RAW_DATA_FOLDER= '../data/unstructured/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = '../data/unstructured/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos extraer los datos de nuestros datos no estructurados a un formato más legible para que el modelo los entienda. La herramienta Document Intelligence nos ayuda a hacerlo aprovechando los modelos de diseño preconstruidos. Aquí, trabajamos principalmente con archivos PDF, pero también podríamos tener formatos JPG y PNG que la herramienta Document Intelligence también admite.\n",
    "\n",
    "Para cada documento, queremos especificar la forma en que se extrae la información. Por ejemplo, en este caso de uso, cada documento tiene muchas páginas. Para hacer un seguimiento de las páginas, las almacenamos en el campo page_number. También queremos extraer el contenido de cada página y colocarlo en un campo page_context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_single_file(file_name: str):\n",
    "    not_completed = True\n",
    "    while not_completed:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            not_completed=False\n",
    "    result = poller.result()\n",
    "    return get_page_content(file_name, result)\n",
    "\n",
    "def extract_files( folder_name: str, destination_folder_name: str):\n",
    "    os.makedirs(destination_folder_name, exist_ok=True)\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file[-3:].upper() in ['PDF','JPG','PNG']:\n",
    "            print('Processing file:', file, end='')\n",
    "        \n",
    "            page_content = extract_local_single_file(os.path.join(folder_name, file))\n",
    "            output_file = os.path.join(destination_folder_name, file[:-3] +'json')\n",
    "            print(f'  write output to {output_file}')\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(json.dumps(page_content))\n",
    "\n",
    "\n",
    "def get_page_content(file_name:str, result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return {'filename':file_name, 'content':page_content}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf"
     ]
    },
    {
     "ename": "ServiceRequestError",
     "evalue": "No connection adapters were found for '/formrecognizer/documentModels/prebuilt-layout:analyze?stringIndexType=unicodeCodePoint&api-version=2022-08-31'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mextract_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_DATA_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXTRACTED_DATA_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mextract_files\u001b[0;34m(folder_name, destination_folder_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPDF\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJPG\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPNG\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing file:\u001b[39m\u001b[38;5;124m'\u001b[39m, file, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     page_content \u001b[38;5;241m=\u001b[39m \u001b[43mextract_local_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(destination_folder_name, file[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  write output to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mextract_local_single_file\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m not_completed:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m         poller \u001b[38;5;241m=\u001b[39m \u001b[43mdocument_analysis_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_analyze_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprebuilt-layout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         not_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m result \u001b[38;5;241m=\u001b[39m poller\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/tracing/decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/ai/formrecognizer/_document_analysis_client.py:126\u001b[0m, in \u001b[0;36mDocumentAnalysisClient.begin_analyze_document\u001b[0;34m(self, model_id, document, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_id:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id cannot be None or empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_analyze_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43manalyze_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/octet-stream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstring_index_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43municodeCodePoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/ai/formrecognizer/_generated/_operations_mixin.py:170\u001b[0m, in \u001b[0;36mFormRecognizerClientOperationsMixin.begin_analyze_document\u001b[0;34m(self, model_id, pages, locale, string_index_type, analyze_request, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m mixin_instance\u001b[38;5;241m.\u001b[39m_serialize\u001b[38;5;241m.\u001b[39mclient_side_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    169\u001b[0m mixin_instance\u001b[38;5;241m.\u001b[39m_deserialize \u001b[38;5;241m=\u001b[39m Deserializer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models_dict(api_version))\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmixin_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_analyze_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring_index_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalyze_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/tracing/decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/ai/formrecognizer/_generated/v2022_08_31/operations/_form_recognizer_client_operations.py:576\u001b[0m, in \u001b[0;36mFormRecognizerClientOperationsMixin.begin_analyze_document\u001b[0;34m(self, model_id, pages, locale, string_index_type, analyze_request, content_type, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m cont_token \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuation_token\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# type: Optional[str]\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cont_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 576\u001b[0m     raw_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze_document_initial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstring_index_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_index_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43manalyze_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalyze_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mz\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_map\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_long_running_output\u001b[39m(pipeline_response):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/ai/formrecognizer/_generated/v2022_08_31/operations/_form_recognizer_client_operations.py:499\u001b[0m, in \u001b[0;36mFormRecognizerClientOperationsMixin._analyze_document_initial\u001b[0;34m(self, model_id, pages, locale, string_index_type, analyze_request, content_type, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m path_format_arguments \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialize\u001b[38;5;241m.\u001b[39murl(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself._config.endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mendpoint, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m, skip_quote\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    496\u001b[0m }\n\u001b[1;32m    497\u001b[0m request\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mformat_url(request\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_format_arguments)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m pipeline_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore # pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m202\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:240\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[1;32m    239\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 96 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/policies/_redirect.py:204\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    202\u001b[0m original_domain \u001b[38;5;241m=\u001b[39m get_domain(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 204\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/policies/_retry.py:573\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    571\u001b[0m                 is_response_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    572\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/policies/_retry.py:551\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[1;32m    550\u001b[0m request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 551\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_retry(retry_settings, response):\n\u001b[1;32m    553\u001b[0m     retry_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(retry_settings, response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 96 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:96\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     94\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/_base.py:128\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m:param request: The PipelineRequest object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m cleanup_kwargs_for_transport(request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PipelineResponse(\n\u001b[1;32m    127\u001b[0m     request\u001b[38;5;241m.\u001b[39mhttp_request,\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    129\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/azure/core/pipeline/transport/_requests_basic.py:409\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[0;34m(self, request, proxies, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     error \u001b[38;5;241m=\u001b[39m ServiceRequestError(err, error\u001b[38;5;241m=\u001b[39merr)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_rest(request):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_requests_basic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RestRequestsTransportResponse\n",
      "\u001b[0;31mServiceRequestError\u001b[0m: No connection adapters were found for '/formrecognizer/documentModels/prebuilt-layout:analyze?stringIndexType=unicodeCodePoint&api-version=2022-08-31'"
     ]
    }
   ],
   "source": [
    "extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más Sobre Nuestros Datos\n",
    "\n",
    "En este tutorial, examinaremos varios artículos de investigación sobre temas de LLM en documentos PDF. Esto incluye temas como:\n",
    "\n",
    "- Autoprompting (prompting automático)\n",
    "- Chain of thought prompting (prompting de cadena de pensamiento)\n",
    "- precise zero shot dense retrival (recuperación densa precisa de cero disparos)\n",
    "- y más. \n",
    "\n",
    "Este conjunto de datos contiene varios formatos no estructurados, como texto, tablas, gráficos y fórmulas.\n",
    "\n",
    "## Descripción de los Datos\n",
    "\n",
    "El esquema relevante para nuestro trabajo de hoy consiste en:\n",
    "\n",
    "- document_id\n",
    "- document_name\n",
    "- file_path\n",
    "- page_number\n",
    "- page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in os.listdir(EXTRACTED_DATA_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_DATA_FOLDER, file)) as f:\n",
    "        page_content= json.loads(f.read())\n",
    "    documents.extend(\n",
    "        [\n",
    "            {\n",
    "                'document_id':page_content['filename'].split('\\\\')[-1].split('.')[0] + '-' + str(page['page_number']),\n",
    "                'document_name':page_content['filename'].split('\\\\')[-1],\n",
    "                'file_path':page_content['filename'],              \n",
    "                'page_number':page['page_number'],\n",
    "                'page_text':page['page_content']\n",
    "            }\n",
    "            for page in page_content['content']\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta sección se centrará en AI Search y los siguientes temas:\n",
    "\n",
    "1. Crear un índice de cliente\n",
    "2. Definir los campos del índice con los atributos necesarios\n",
    "3. Crear una configuración semántica\n",
    "4. Cargar nuestro índice con las páginas de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_DOC_INDEX_NAME\")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"document_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"document_id\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"document_name\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Aquí vemos a Azure AI Search en acción! Podemos recuperar los documentos más relevantes de todos con los que estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is prompt tuning?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "citations = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    citations.append(result['document_name'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = pd.DataFrame(page_chunks, columns = [\"page_chunks\"]) #datframe with document chunks\n",
    "embed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tengamos los documentos más relevantes, crearemos embeddings para todos los fragmentos de página. Esto nos ayudará a encontrar los documentos más similares a nuestra consulta de usuario dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = get_embedding(query, engine=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "User Query: ```{query}```\n",
    "List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt, chat_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "    Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "    User Query: ```{query}```\n",
    "    List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt, chat_model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_search(\"How does prompt tuning work?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = query_search(\"what is a fully zero-shot dense retrieval system?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
